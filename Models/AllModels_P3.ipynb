{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa59fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "OUTPUT_DIR = '../output'\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True) \n",
    "\n",
    "# --- 1. LOAD DATA ---\n",
    "print(\"Loading Data...\")\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "\n",
    "# Combine for consistent processing\n",
    "train['is_train'] = 1\n",
    "test['is_train'] = 0\n",
    "test['retention_status'] = 'Unknown'\n",
    "combined = pd.concat([train, test], axis=0)\n",
    "\n",
    "\n",
    "print(\"Applying Forensic Cleaning Pipeline...\")\n",
    "\n",
    "\n",
    "cols_to_drop = ['founder_id', 'founder_role', 'leadership_scope', \n",
    "                'founder_visibility', 'innovation_support', 'team_size_category']\n",
    "combined = combined.drop(columns=cols_to_drop)\n",
    "\n",
    "#  FIX SKEW\n",
    "combined['monthly_revenue_generated'] = np.log1p(combined['monthly_revenue_generated'].fillna(combined['monthly_revenue_generated'].median()))\n",
    "\n",
    "#  IMPUTE MISSING VALUES\n",
    "combined['years_since_founding'] = combined['years_since_founding'].fillna(combined['years_since_founding'].median())\n",
    "combined['num_dependents'] = combined['num_dependents'].fillna(combined['num_dependents'].mode()[0])\n",
    "combined['work_life_balance_rating'] = combined['work_life_balance_rating'].fillna('Unknown')\n",
    "combined['venture_satisfaction'] = combined['venture_satisfaction'].fillna('Unknown')\n",
    "\n",
    "# FEATURE ENGINEERING\n",
    "combined['founder_age'] = combined['founder_age'].clip(lower=18)\n",
    "combined['start_age'] = combined['founder_age'] - combined['years_with_startup']\n",
    "\n",
    "#  ORDINAL ENCODING\n",
    "rating_map = {'Unknown': 0, 'Low': 1, 'Poor': 1, 'Below Average': 2, \n",
    "              'Fair': 3, 'Medium': 3, 'Average': 3, \n",
    "              'Good': 4, 'High': 4, 'Very High': 5, 'Excellent': 5}\n",
    "for col in ['work_life_balance_rating', 'venture_satisfaction', 'startup_performance_rating', 'startup_reputation']:\n",
    "    combined[col] = combined[col].map(rating_map).fillna(0)\n",
    "\n",
    "stage_map = {'Entry': 1, 'Mid': 2, 'Senior': 3, 'Growth': 3, 'Established': 4}\n",
    "combined['startup_stage'] = combined['startup_stage'].map(stage_map).fillna(1)\n",
    "\n",
    "binary_map = {'No': 0, 'Yes': 1}\n",
    "for col in ['working_overtime', 'remote_operations']:\n",
    "    combined[col] = combined[col].map(binary_map)\n",
    "\n",
    "#  ONE-HOT ENCODING\n",
    "combined = pd.get_dummies(combined, columns=['founder_gender', 'education_background', 'personal_status'], drop_first=True)\n",
    "\n",
    "\n",
    "print(\"Scaling Data...\")\n",
    "train_final = combined[combined['is_train'] == 1].drop(columns=['is_train'])\n",
    "test_final = combined[combined['is_train'] == 0].drop(columns=['is_train', 'retention_status'])\n",
    "\n",
    "# Target Mapping\n",
    "target_map = {'Stayed': 0, 'Left': 1}\n",
    "y = train_final['retention_status'].map(target_map)\n",
    "X = train_final.drop(columns=['retention_status'])\n",
    "X_submit = test_final[X.columns]\n",
    "\n",
    "# Scale (Critical for SVM/NN)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_submit_scaled = scaler.transform(X_submit)\n",
    "\n",
    "inverse_map = {0: 'Stayed', 1: 'Left'}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610b1589",
   "metadata": {},
   "source": [
    "# Gradient boost, Random Forest, SVM and Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015c821",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models_config = {\n",
    "    \"GradientBoosting\": {\n",
    "        \"estimator\": GradientBoostingClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.05, 0.1],\n",
    "            'max_depth': [3, 4, 5],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"estimator\": RandomForestClassifier(random_state=42),\n",
    "        \"params\": {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [10, 20, None],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        \"estimator\": SVC(random_state=42),\n",
    "        \"params\": {\n",
    "            'C': [0.1, 1, 10],\n",
    "            'kernel': ['rbf'], \n",
    "            'gamma': ['scale', 'auto']\n",
    "        }\n",
    "    },\n",
    "    \"NeuralNetwork\": {\n",
    "        \"estimator\": MLPClassifier(max_iter=500, random_state=42),\n",
    "        \"params\": {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'alpha': [0.0001, 0.001],\n",
    "            'learning_rate_init': [0.001, 0.01]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"   STARTING MULTI-MODEL GENERATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for name, config in models_config.items():\n",
    "    print(f\"\\nProcessing Model: {name}...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    # 1. Hyperparameter Tuning (Random Search)\n",
    "    search = RandomizedSearchCV(\n",
    "        estimator=config['estimator'],\n",
    "        param_distributions=config['params'],\n",
    "        n_iter=10,\n",
    "        cv=3,\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,\n",
    "        random_state=42,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    search.fit(X_scaled, y)\n",
    "    best_model = search.best_estimator_\n",
    "    \n",
    "    print(f\"  -> Best Params: {search.best_params_}\")\n",
    "    print(f\"  -> Best CV Score: {search.best_score_:.4%}\")\n",
    "    \n",
    "    # 2. Predict on Test Set using the BEST model\n",
    "    preds = best_model.predict(X_submit_scaled)\n",
    "    \n",
    "    # 3. Save CSV to OUTPUT_DIR\n",
    "    filename = f\"{OUTPUT_DIR}/submission_{name}.csv\"\n",
    "    submission = pd.DataFrame({\n",
    "        'founder_id': test['founder_id'],\n",
    "        'retention_status': [inverse_map[p] for p in preds]\n",
    "    })\n",
    "    submission.to_csv(filename, index=False)\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"  -> Saved to '{filename}' (Time: {elapsed:.1f}s)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(f\"   ALL FILES GENERATED IN {OUTPUT_DIR}/\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a715b5",
   "metadata": {},
   "source": [
    "# Using all the csv generated to Ensemble and get better accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316cd510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.stats import mode\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "OUTPUT_DIR = '../output'\n",
    "files = [\n",
    "    'submission_GradientBoosting.csv',\n",
    "    'submission_RandomForest.csv',\n",
    "    'submission_SVM.csv',\n",
    "    'submission_NeuralNetwork.csv'\n",
    "]\n",
    "\n",
    "print(\"Loading predictions...\")\n",
    "dfs = []\n",
    "for f in files:\n",
    "    path = os.path.join(OUTPUT_DIR, f)\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_csv(path)\n",
    "        # Convert to numbers for math (Stayed=0, Left=1)\n",
    "        df['retention_numeric'] = df['retention_status'].map({'Stayed': 0, 'Left': 1})\n",
    "        dfs.append(df['retention_numeric'].values)\n",
    "        print(f\"Loaded: {f}\")\n",
    "    else:\n",
    "        print(f\"Warning: {f} not found. Skipping.\")\n",
    "\n",
    "if not dfs:\n",
    "    print(\"No files found!\")\n",
    "    exit()\n",
    "\n",
    "# --- HARD VOTING ---\n",
    "# We stack the predictions and take the \"Mode\" (Majority Vote)\n",
    "stacked_preds = np.array(dfs)\n",
    "\n",
    "final_votes, _ = mode(stacked_preds, axis=0)\n",
    "\n",
    "# Flatten the array\n",
    "final_votes = final_votes.ravel()\n",
    "\n",
    "\n",
    "inverse_map = {0: 'Stayed', 1: 'Left'}\n",
    "submission_ensemble = pd.read_csv(os.path.join(OUTPUT_DIR, files[0])) # Load template\n",
    "submission_ensemble['retention_status'] = [inverse_map[p] for p in final_votes]\n",
    "\n",
    "output_path = os.path.join(OUTPUT_DIR, 'submission_Ensemble_Voting.csv')\n",
    "submission_ensemble.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\nâœ… Created Ensemble Submission: {output_path}\")\n",
    "print(\"This file combines the 'wisdom' of all your models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MlLab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
